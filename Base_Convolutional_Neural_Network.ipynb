{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anrOviiFnFdg"
      },
      "outputs": [],
      "source": [
        "!pip install pandas \n",
        "!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as  pd\n",
        "import io\n",
        "from PIL import Image, ImageFilter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#from google.colab import files\n"
      ],
      "metadata": {
        "id": "th9VVL-2nMCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inputting data from .CSV"
      ],
      "metadata": {
        "id": "HnEdXUAvn8f1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Taking input of cloud excel file \n",
        "\n",
        "df = pd.read_csv('clouds_train_small.csv')\n",
        "print(df)"
      ],
      "metadata": {
        "id": "Pdjg9-HXnOEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pulling images from AWS bucket "
      ],
      "metadata": {
        "id": "K_bTUbjfoEVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loadWeatherFile(filename, image_width = -1):\n",
        "  import urllib.request\n",
        "  import io\n",
        "  from PIL import Image\n",
        "  url = \"http://uindy-weathercam.s3.us-east-2.amazonaws.com/\" + filename\n",
        "  fh = urllib.request.urlopen(url)\n",
        "  if image_width == -1:\n",
        "    return Image.open(io.BytesIO(fh.read()))\n",
        "  else:\n",
        "    im = Image.open(io.BytesIO(fh.read()))\n",
        "    width, height = im.size\n",
        "    image_height = image_width / width * height\n",
        "    im = im.resize( (int(image_width), int(image_height)), Image.ANTIALIAS)\n",
        "    return im\n",
        "\n",
        "#Loading the file .jpg name then the labels \n",
        "df = df[['Cloud File', 'Label']]\n",
        "df = df.astype({'Label': 'int32'}) # \n",
        "#if the label is 3 (rain picture) or 5 (bad picture) they're not accepted \n",
        "df = df.loc[(df.Label != 5) & (df.Label != 3)].reset_index(drop=True)\n",
        "df.loc[df.Label == 4, 'Label'] = 3\n",
        "\n",
        "df = df.sample(frac=1, random_state=2).reset_index(drop=True)\n",
        "\n",
        "print(df)\n",
        "print(df.dtypes)"
      ],
      "metadata": {
        "id": "7hdNcq2jnPhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from multiprocessing import Pool, TimeoutError \n",
        "\n",
        "#load all ofthe files from the bucket, calling the above function\n",
        "def f(filename):\n",
        "    arr = np.array(loadWeatherFile(filename,200))  # <<---------- Image width! Can play with this to resize images for less CPU/GPU power \n",
        "    #making all the pixels from 0-1\n",
        "    arr = arr / 255.0\n",
        "    return arr\n",
        "\n",
        "image_filenames = df[:]['Cloud File'].tolist()\n",
        "with Pool(processes=12) as pool:      \n",
        "    image_arr = pool.map(f, image_filenames)\n",
        "print(\"Done processing images\")\n",
        "   \n",
        "#making the np array of the 2D images \n",
        "image_features = np.asarray(image_arr)\n",
        "print(image_features.shape)\n",
        "\n",
        "#loading the labels so they're with the correct image \n",
        "image_labels = df[:(image_features.shape[0])]['Label']\n",
        "print(image_labels.shape)\n",
        "\n",
        "#showing the first image in the array \n",
        "plt.imshow(image_features[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kKIctaWJnvO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creation of Model "
      ],
      "metadata": {
        "id": "ATI3qVI2oI3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creation of the model\n",
        "\n",
        "model = None \n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(filters=4, kernel_size=5, activation='relu',\n",
        "                               padding='same'),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=2, strides=3),\n",
        "    tf.keras.layers.Conv2D(filters=4, kernel_size=5,\n",
        "                               activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=2, strides=3),\n",
        "    tf.keras.layers.Conv2D(filters=4, kernel_size=5,\n",
        "                               activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=2, strides=3),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(928, activation='relu' ),\n",
        "    tf.keras.layers.Dense(256, activation='relu' ),\n",
        "    tf.keras.layers.Dense(128, activation='relu' ),\n",
        "    tf.keras.layers.Dense(units=4),#, kernel_initializer=init)\n",
        "])"
      ],
      "metadata": {
        "id": "CwUX8fZ8nRp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compilation of Model"
      ],
      "metadata": {
        "id": "kMceQdwYoLFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Attemp to predict something - this will allow the model to know the input shape for the summary later!\n",
        "model.predict(image_features) \n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=['accuracy'],\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)) \n",
        "print(model.summary())\n",
        "history = None\n"
      ],
      "metadata": {
        "id": "f-_DgWEnnXN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "lypJpQqxoOab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "history = model.fit(\n",
        "    image_features,\n",
        "    image_labels,\n",
        "    epochs=20,\n",
        "    # Suppress logging.\n",
        "    verbose=1,\n",
        "    # Calculate validation results on 20% of the training data.\n",
        "    validation_split = 0.1,\n",
        "    )"
      ],
      "metadata": {
        "id": "1iv9jqGYnXiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results Graph"
      ],
      "metadata": {
        "id": "OfhuWKIBoQ50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(history.history.keys())\n",
        "\n",
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "  plt.ylim([0, 2])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "plot_loss(history)"
      ],
      "metadata": {
        "id": "y-vEV2TGngo1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}